name: Daily Microsoft & Meta Jobs Scraper

on:
  schedule:
    # Run daily at 7:00 AM UTC (adjust timezone as needed)
    - cron: '0 7 * * *'

  # Allow manual triggering
  workflow_dispatch:

# Grant necessary permissions for the workflow
permissions:
  contents: write
  actions: read

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch full history for proper git operations
        fetch-depth: 0
        # Use a personal access token for pushing changes
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Chrome
      run: |
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Run Microsoft job scraper
      run: |
        python ms-jobs-daily-scrapper.py
      env:
        # Set display for headless Chrome
        DISPLAY: :99

    - name: Run Meta job scraper
      run: |
        python meta-jobs-daily-scraper.py
      env:
        # Set display for headless Chrome
        DISPLAY: :99

    - name: Configure Git
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

    - name: Check for changes
      id: git-check
      run: |
        git add .
        if git diff --staged --quiet; then
          echo "changes=false" >> $GITHUB_OUTPUT
        else
          echo "changes=true" >> $GITHUB_OUTPUT
        fi

    - name: Commit and push changes
      if: steps.git-check.outputs.changes == 'true'
      run: |
        git commit -m "ðŸ¤– Daily job scraping update - $(date +'%Y-%m-%d %H:%M UTC')"
        git push

    - name: Create summary
      if: always()
      run: |
        python - <<'PY'
        import json
        import os
        import glob
        from pathlib import Path
        from datetime import datetime

        cfg = json.load(open('config.json'))
        out_path = os.environ.get('GITHUB_STEP_SUMMARY', None)
        if not out_path:
            # Fallback to stdout if the environment variable is missing
            import sys
            _out = sys.stdout
        else:
            _out = open(out_path, 'a', encoding='utf-8')

        def write(s=''):
            _out.write(s + '\n')

        company_names = ', '.join(c.get('companyName', '') for c in cfg.get('companies', []))
        write(f"## Daily {company_names} Jobs Scraping Results")
        write()
        write(f"**Date:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}")
        write()

        # Look for any folders that end with '-jobs' and try to match company name to folder name (case-insensitive)
        jobs_folders = [p for p in glob.glob('*-jobs')]

        for comp in cfg.get('companies', []):
            name = comp.get('companyName')
            write(f"### ðŸ¢ {name} Jobs")

            # find best matching folder
            candidates = [p for p in jobs_folders if name.lower() in p.lower()]
            folder = Path(candidates[0]) if candidates else None

            if folder and folder.exists():
                # total ids file (look for *_ids.json)
                ids_files = list(folder.glob('*_ids.json'))
                if ids_files:
                    try:
                        data = json.load(open(ids_files[0], encoding='utf-8'))
                        write(f"**Total Jobs in Database:** {len(data)}")
                    except Exception:
                        write(f"**Total Jobs in Database:** (could not read {ids_files[0].name})")

                # details file (look for *_details.json)
                details_files = list(folder.glob('*_details.json'))
                if details_files:
                    try:
                        data = json.load(open(details_files[0], encoding='utf-8'))
                        write(f"**Jobs with Details:** {len(data)}")
                    except Exception:
                        write(f"**Jobs with Details:** (could not read {details_files[0].name})")

                # filtered / avoid files
                avoid_files = [p for p in folder.glob('*avoid*.json')]
                if avoid_files:
                    write()
                    write("**Filtered Categories:**")
                    try:
                        data = json.load(open(avoid_files[0], encoding='utf-8'))
                        if isinstance(data, dict):
                            for k, v in data.items():
                                count = 0
                                if isinstance(v, dict):
                                    count = len(v.get('job_ids', []))
                                write(f"- {k}: {count}")
                        else:
                            write(f"- (unexpected format in {avoid_files[0].name})")
                    except Exception:
                        write(f"- (could not read {avoid_files[0].name})")

                # jobs_by_date listing
                bydate = folder / 'jobs_by_date'
                if bydate.exists() and bydate.is_dir():
                    write()
                    write(f"**{name} Jobs by Date Files:**")
                    files = sorted([f.name for f in bydate.glob('*.json')], reverse=True)[:5]
                    for fn in files:
                        write(f"- {fn}")
            else:
                write(f"(No job folder found for {name})")

            write()

        if out_path:
            _out.close()
        PY

    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: job-scraping-results-${{ github.run_number }}
        path: |
          Microsoft-jobs/ms_job_ids.json
          Microsoft-jobs/ms_job_details.json
          Microsoft-jobs/ms_job_avoid_hits_by_field.json
          Microsoft-jobs/jobs_by_date/
          Meta-jobs/meta_job_ids.json
          Meta-jobs/meta_job_details.json
          Meta-jobs/jobs_by_date/
        retention-days: 30

    - name: Notify on failure
      if: failure()
      run: |
        echo "âŒ Job scraping failed. Check the logs for details." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Possible issues:**" >> $GITHUB_STEP_SUMMARY
        echo "- Website structure changes" >> $GITHUB_STEP_SUMMARY
        echo "- Network connectivity issues" >> $GITHUB_STEP_SUMMARY
        echo "- Chrome/Selenium compatibility issues" >> $GITHUB_STEP_SUMMARY
        echo "- Rate limiting from the target website" >> $GITHUB_STEP_SUMMARY
