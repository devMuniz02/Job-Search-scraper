{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f09b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] loading: jobs_ms.json\n",
      "[DB] existing records: 389\n",
      "[SCRAPE] paginate up to 10 pages (20 per page)…\n",
      "[PAGE 1] cards found: 20\n",
      "[PAGE 2] cards found: 20\n",
      "[PAGE 3] cards found: 20\n",
      "[PAGE 4] cards found: 20\n",
      "[PAGE 5] cards found: 20\n",
      "[PAGE 6] cards found: 20\n",
      "[PAGE 7] cards found: 20\n",
      "[PAGE 8] cards found: 20\n",
      "[PAGE 9] cards found: 20\n",
      "[PAGE 10] cards found: 20\n",
      "[SCRAPE] total rows scraped: 200\n",
      "[DB] new rows added: 20\n",
      "[DB] saving to: jobs_ms.json\n",
      "\n",
      "[PREVIEW] first 10 entries:\n",
      "- Principal Applied Scientist | 1881669 | None | https://jobs.careers.microsoft.com/global/en/job/1881669/\n",
      "- Senior Product Marketing Manager, Identity | 1879566 | None | https://jobs.careers.microsoft.com/global/en/job/1879566/\n",
      "- Industry Advisory - Microsoft Discovery | 1881248 | None | https://jobs.careers.microsoft.com/global/en/job/1881248/\n",
      "- Sales Strategy Enablement - Higher Education | 1880149 | None | https://jobs.careers.microsoft.com/global/en/job/1880149/\n",
      "- Research Intern - MSR Systems Research Group - Redmond | 1883457 | None | https://jobs.careers.microsoft.com/global/en/job/1883457/\n",
      "- Data Center Technician Manager (Night Shift) | 1837771 | None | https://jobs.careers.microsoft.com/global/en/job/1837771/\n",
      "- Software Engineer II | 1878935 | None | https://jobs.careers.microsoft.com/global/en/job/1878935/\n",
      "- Associate Creative Director | 1876605 | None | https://jobs.careers.microsoft.com/global/en/job/1876605/\n",
      "- Business Analytics Associate | 1881287 | None | https://jobs.careers.microsoft.com/global/en/job/1881287/\n",
      "- Security Business Strategist | 1841942 | None | https://jobs.careers.microsoft.com/global/en/job/1841942/\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade \"selenium>=4.20\" requests beautifulsoup4\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "SEARCH_URL = (\"https://jobs.careers.microsoft.com/global/en/search\"\n",
    "              \"?lc=Mexico&lc=United%20States&l=en_us&pg=1&pgSz=20&o=Recent&flt=true\")\n",
    "\n",
    "DB_PATH = \"jobs_ms.json\"  # persistent JSON (dict keyed by job_id or url)\n",
    "MAX_PAGES = 10\n",
    "PAGE_LOAD_TIMEOUT = 60\n",
    "WAIT_PER_PAGE = 25\n",
    "DELAY_AFTER_NEXT = 1.2\n",
    "\n",
    "# Optional: if Selenium Manager is blocked and you have a local chromedriver, set this path:\n",
    "LOCAL_CHROMEDRIVER = \"\"  # e.g., r\"C:\\Tools\\chromedriver\\chromedriver.exe\"\n",
    "\n",
    "JOB_ID_FROM_ARIA = re.compile(r\"Job item\\s+(\\d+)\")\n",
    "ISO_DATE_RE = re.compile(r\"(20\\d{2})-(\\d{2})-(\\d{2})\")\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"MS-Careers-Scraper/1.5 (+you@example.com)\"})\n",
    "\n",
    "\n",
    "def launch_chrome():\n",
    "    opts = ChromeOptions()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1400,2000\")\n",
    "    if LOCAL_CHROMEDRIVER:\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        return webdriver.Chrome(service=Service(LOCAL_CHROMEDRIVER), options=opts)\n",
    "    return webdriver.Chrome(options=opts)  # Selenium Manager\n",
    "\n",
    "\n",
    "def with_page(url: str, page: int) -> str:\n",
    "    \"\"\"Fallback: set pg=<page> in query if clicking Next fails.\"\"\"\n",
    "    parts = list(urlparse(url))\n",
    "    q = parse_qs(parts[4], keep_blank_values=True)\n",
    "    q[\"pg\"] = [str(page)]\n",
    "    parts[4] = urlencode(q, doseq=True)\n",
    "    return urlunparse(parts)\n",
    "\n",
    "\n",
    "def find_cards(driver):\n",
    "    return driver.find_elements(By.CSS_SELECTOR, 'div[role=\"listitem\"]')\n",
    "\n",
    "\n",
    "def title_from_card(card):\n",
    "    try:\n",
    "        h2 = card.find_element(By.CSS_SELECTOR, \"h2\")\n",
    "        t = (h2.text or \"\").strip()\n",
    "        if t:\n",
    "            return t\n",
    "    except Exception:\n",
    "        pass\n",
    "    txt = (card.text or \"\").strip()\n",
    "    return txt.splitlines()[0].strip() if txt else None\n",
    "\n",
    "\n",
    "def job_id_from_card(card):\n",
    "    aria = card.get_attribute(\"aria-label\") or \"\"\n",
    "    m = JOB_ID_FROM_ARIA.search(aria)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    try:\n",
    "        outer = card._parent.execute_script(\"return arguments[0].outerHTML;\", card)\n",
    "    except Exception:\n",
    "        outer = \"\"\n",
    "    m2 = JOB_ID_FROM_ARIA.search(outer or \"\")\n",
    "    return m2.group(1) if m2 else None\n",
    "\n",
    "\n",
    "def link_from_card(card, job_id):\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[href*=\"/global/en/job/\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            return href\n",
    "    except Exception:\n",
    "        pass\n",
    "    return f\"https://jobs.careers.microsoft.com/global/en/job/{job_id}/\" if job_id else None\n",
    "\n",
    "\n",
    "def parse_date_posted_from_detail(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    for tag in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
    "        raw = tag.string or \"\"\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except Exception:\n",
    "            continue\n",
    "        items = data if isinstance(data, list) else [data]\n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and item.get(\"@type\") in {\"JobPosting\", \"Posting\"}:\n",
    "                dp = item.get(\"datePosted\") or item.get(\"dateCreated\") or item.get(\"dateModified\")\n",
    "                if dp:\n",
    "                    m = ISO_DATE_RE.search(dp)\n",
    "                    if m:\n",
    "                        return f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
    "\n",
    "    m2 = ISO_DATE_RE.search(html_text)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)}-{m2.group(2)}-{m2.group(3)}\"\n",
    "\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    if \"Today\" in text:\n",
    "        return dt.date.today().isoformat()\n",
    "    return None\n",
    "\n",
    "\n",
    "def click_next_if_possible(driver) -> bool:\n",
    "    \"\"\"\n",
    "    Try several selectors to click the 'Next' pagination button.\n",
    "    Returns True if we clicked something that looked like Next.\n",
    "    \"\"\"\n",
    "    selectors = [\n",
    "        (By.CSS_SELECTOR, 'button[aria-label*=\"Next\"]:not([disabled]):not([aria-disabled=\"true\"])'),\n",
    "        (By.XPATH, \"//button[(contains(., 'Next') or contains(@aria-label, 'Next')) and not(@disabled) and not(@aria-disabled='true')]\"),\n",
    "        (By.XPATH, \"//a[(contains(., 'Next') or contains(@aria-label, 'Next')) and not(contains(@class,'disabled'))]\"),\n",
    "    ]\n",
    "    for by, sel in selectors:\n",
    "        try:\n",
    "            btn = driver.find_element(by, sel)\n",
    "            # Some UIs hide Next when not applicable; verify displayed & enabled\n",
    "            if not btn.is_displayed():\n",
    "                continue\n",
    "            if btn.get_attribute(\"disabled\") or btn.get_attribute(\"aria-disabled\") == \"true\":\n",
    "                continue\n",
    "            btn.click()\n",
    "            return True\n",
    "        except Exception:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "\n",
    "def wait_for_new_page(driver, prev_ids, timeout=12) -> bool:\n",
    "    \"\"\"\n",
    "    After clicking Next, wait until we see a card with a job_id not in prev_ids,\n",
    "    or DOM count changes. Returns True if changed, False if timeout.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    last_count = len(prev_ids)\n",
    "    while time.time() - t0 < timeout:\n",
    "        time.sleep(0.8)\n",
    "        cards = find_cards(driver)\n",
    "        # attach driver reference for outerHTML extraction\n",
    "        for c in cards:\n",
    "            try:\n",
    "                c._parent = driver\n",
    "            except Exception:\n",
    "                pass\n",
    "        curr_ids = set()\n",
    "        for c in cards:\n",
    "            jid = job_id_from_card(c)\n",
    "            if jid:\n",
    "                curr_ids.add(jid)\n",
    "        if len(cards) != last_count or (curr_ids - prev_ids):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def scrape_paginated(max_pages=MAX_PAGES):\n",
    "    driver = launch_chrome()\n",
    "    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "    wait = WebDriverWait(driver, WAIT_PER_PAGE)\n",
    "\n",
    "    # Start at pg=1\n",
    "    driver.get(SEARCH_URL)\n",
    "    try:\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[role=\"listitem\"]')))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    all_rows = []\n",
    "    seen_global_ids = set()\n",
    "\n",
    "    current_page = 1\n",
    "    while current_page <= max_pages:\n",
    "        # Collect this page’s 20 (or fewer)\n",
    "        cards = find_cards(driver)\n",
    "        for c in cards:\n",
    "            try:\n",
    "                c._parent = driver\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        print(f\"[PAGE {current_page}] cards found: {len(cards)}\")\n",
    "\n",
    "        page_ids = set()\n",
    "        for card in cards:\n",
    "            name = title_from_card(card)\n",
    "            jid  = job_id_from_card(card)\n",
    "            if jid:\n",
    "                page_ids.add(jid)\n",
    "                if jid in seen_global_ids:\n",
    "                    continue\n",
    "            url  = link_from_card(card, jid)\n",
    "            date_posted = None\n",
    "            if url:\n",
    "                try:\n",
    "                    r = session.get(url, timeout=25, allow_redirects=True)\n",
    "                    date_posted = parse_date_posted_from_detail(r.text)\n",
    "                    url = r.url\n",
    "                except Exception:\n",
    "                    pass\n",
    "            all_rows.append({\n",
    "                \"name\": name,\n",
    "                \"job_id\": jid,\n",
    "                \"url\": url,\n",
    "                \"date_posted\": date_posted\n",
    "            })\n",
    "            if jid:\n",
    "                seen_global_ids.add(jid)\n",
    "\n",
    "        # If fewer than 20, we’re done\n",
    "        if len(cards) < 20:\n",
    "            break\n",
    "\n",
    "        # Try clicking Next; if that fails, try navigating with pg=+1\n",
    "        clicked = click_next_if_possible(driver)\n",
    "        if clicked:\n",
    "            # wait for a change\n",
    "            changed = wait_for_new_page(driver, page_ids, timeout=12)\n",
    "            if not changed:\n",
    "                # fallback: explicit navigation\n",
    "                next_url = with_page(SEARCH_URL, current_page + 1)\n",
    "                driver.get(next_url)\n",
    "                try:\n",
    "                    WebDriverWait(driver, WAIT_PER_PAGE).until(\n",
    "                        EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[role=\"listitem\"]'))\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "        else:\n",
    "            # no Next button detected; fallback to URL pg=+1\n",
    "            next_url = with_page(SEARCH_URL, current_page + 1)\n",
    "            driver.get(next_url)\n",
    "            try:\n",
    "                WebDriverWait(driver, WAIT_PER_PAGE).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[role=\"listitem\"]'))\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        time.sleep(DELAY_AFTER_NEXT)\n",
    "        current_page += 1\n",
    "\n",
    "    driver.quit()\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "# -------- persistence helpers -------- #\n",
    "\n",
    "def load_db(path: str) -> dict:\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, dict):\n",
    "            return data\n",
    "        out = {}\n",
    "        for row in data:\n",
    "            key = (row.get(\"job_id\") or row.get(\"url\"))\n",
    "            if key:\n",
    "                out[str(key)] = row\n",
    "        return out\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def save_db_atomic(path: str, data: dict):\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "    fd, tmp_path = tempfile.mkstemp(prefix=\"jobs_\", suffix=\".json\", dir=os.path.dirname(os.path.abspath(path)))\n",
    "    os.close(fd)\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "\n",
    "def upsert_rows(db: dict, rows: list) -> int:\n",
    "    added = 0\n",
    "    for row in rows:\n",
    "        key = str(row.get(\"job_id\") or row.get(\"url\"))\n",
    "        if not key:\n",
    "            continue\n",
    "        if key not in db:\n",
    "            db[key] = row\n",
    "            added += 1\n",
    "        else:\n",
    "            old = db[key]\n",
    "            for fld in (\"name\", \"url\", \"date_posted\"):\n",
    "                if row.get(fld):\n",
    "                    old[fld] = row[fld]\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"[DB] loading: {DB_PATH}\")\n",
    "    db = load_db(DB_PATH)\n",
    "    print(f\"[DB] existing records: {len(db)}\")\n",
    "\n",
    "    print(f\"[SCRAPE] paginate up to {MAX_PAGES} pages (20 per page)…\")\n",
    "    rows = scrape_paginated(max_pages=MAX_PAGES)\n",
    "    print(f\"[SCRAPE] total rows scraped: {len(rows)}\")\n",
    "\n",
    "    added = upsert_rows(db, rows)\n",
    "    print(f\"[DB] new rows added: {added}\")\n",
    "\n",
    "    print(f\"[DB] saving to: {DB_PATH}\")\n",
    "    save_db_atomic(DB_PATH, db)\n",
    "\n",
    "    print(\"\\n[PREVIEW] first 10 entries:\")\n",
    "    for r in list(db.values())[:10]:\n",
    "        print(f\"- {r.get('name')} | {r.get('job_id')} | {r.get('date_posted')} | {r.get('url')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6095db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 jobs loaded.\n"
     ]
    }
   ],
   "source": [
    "# read json\n",
    "jobs = []\n",
    "with open(DB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    jobs = json.load(f)\n",
    "print(len(jobs), \"jobs loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3845474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DETAILS] processing 408 job pages …\n",
      "[1/408] SKIP already saved: 1881669\n",
      "[2/408] SKIP already saved: 1879566\n",
      "[3/408] SKIP already saved: 1881248\n",
      "[4/408] SKIP already saved: 1880149\n",
      "[5/408] SKIP already saved: 1883457\n",
      "[6/408] SKIP already saved: 1837771\n",
      "[7/408] SKIP already saved: 1878935\n",
      "[8/408] SKIP already saved: 1876605\n",
      "[9/408] SKIP already saved: 1881287\n",
      "[10/408] SKIP already saved: 1841942\n",
      "[11/408] SKIP already saved: 1883443\n",
      "[12/408] SKIP already saved: 1881809\n",
      "[13/408] SKIP already saved: 1882128\n",
      "[14/408] SKIP already saved: 1878004\n",
      "[15/408] SKIP already saved: 1860231\n",
      "[16/408] SKIP already saved: 1881109\n",
      "[17/408] SKIP already saved: 1880623\n",
      "[18/408] SKIP already saved: 1882874\n",
      "[19/408] SKIP already saved: 1875579\n",
      "[20/408] SKIP already saved: 1883136\n",
      "[21/408] SKIP already saved: 1883354\n",
      "[22/408] SKIP already saved: 1858524\n",
      "[23/408] SKIP already saved: 1880866\n",
      "[24/408] SKIP already saved: 1881872\n",
      "[25/408] SKIP already saved: 1839391\n",
      "[26/408] SKIP already saved: 1883330\n",
      "[27/408] SKIP already saved: 1854615\n",
      "[28/408] SKIP already saved: 1875055\n",
      "[29/408] SKIP already saved: 1880646\n",
      "[30/408] SKIP already saved: 1881367\n",
      "[31/408] SKIP already saved: 1882249\n",
      "[32/408] SKIP already saved: 1876002\n",
      "[33/408] SKIP already saved: 1862877\n",
      "[34/408] SKIP already saved: 1880527\n",
      "[35/408] SKIP already saved: 1872469\n",
      "[36/408] SKIP already saved: 1881646\n",
      "[37/408] SKIP already saved: 1839012\n",
      "[38/408] SKIP already saved: 1881192\n",
      "[39/408] SKIP already saved: 1878027\n",
      "[40/408] SKIP already saved: 1874007\n",
      "[41/408] SKIP already saved: 1881096\n",
      "[42/408] SKIP already saved: 1847481\n",
      "[43/408] SKIP already saved: 1879063\n",
      "[44/408] SKIP already saved: 1850508\n",
      "[45/408] SKIP already saved: 1882711\n",
      "[46/408] SKIP already saved: 1873390\n",
      "[47/408] SKIP already saved: 1881637\n",
      "[48/408] SKIP already saved: 1883197\n",
      "[49/408] SKIP already saved: 1878863\n",
      "[50/408] SKIP already saved: 1857933\n",
      "[51/408] SKIP already saved: 1879544\n",
      "[52/408] SKIP already saved: 1873243\n",
      "[53/408] SKIP already saved: 1882255\n",
      "[54/408] SKIP already saved: 1861814\n",
      "[55/408] SKIP already saved: 1872507\n",
      "[56/408] SKIP already saved: 1861593\n",
      "[57/408] SKIP already saved: 1879479\n",
      "[58/408] SKIP already saved: 1883179\n",
      "[59/408] SKIP already saved: 1876781\n",
      "[60/408] SKIP already saved: 1860402\n",
      "[61/408] SKIP already saved: 1881136\n",
      "[62/408] SKIP already saved: 1881837\n",
      "[63/408] SKIP already saved: 1883140\n",
      "[64/408] SKIP already saved: 1840475\n",
      "[65/408] SKIP already saved: 1882728\n",
      "[66/408] SKIP already saved: 1849447\n",
      "[67/408] SKIP already saved: 1852516\n",
      "[68/408] SKIP already saved: 1883009\n",
      "[69/408] SKIP already saved: 1882288\n",
      "[70/408] GET https://jobs.careers.microsoft.com/global/en/job/1882915/\n",
      "   ! attempt 1 failed: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff771d51eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff771d51f10+80288]\n",
      "\t(No symbol) [0x0x7ff771ad02fa]\n",
      "\t(No symbol) [0x0x7ff771b27cd7]\n",
      "\t(No symbol) [0x0x7ff771b27f9c]\n",
      "\t(No symbol) [0x0x7ff771b7ba87]\n",
      "\t(No symbol) [0x0x7ff771b503bf]\n",
      "\t(No symbol) [0x0x7ff771b787fb]\n",
      "\t(No symbol) [0x0x7ff771b50153]\n",
      "\t(No symbol) [0x0x7ff771b18b02]\n",
      "\t(No symbol) [0x0x7ff771b198d3]\n",
      "\tGetHandleVerifier [0x0x7ff77200e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff772008c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff7720286c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff771d6cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff771d750af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff771d5af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff771d5b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff771d410a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffe919de8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffe933a8d9c+44]\n",
      "\n",
      "   ! attempt 2 failed: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff771d51eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff771d51f10+80288]\n",
      "\t(No symbol) [0x0x7ff771ad02fa]\n",
      "\t(No symbol) [0x0x7ff771b27cd7]\n",
      "\t(No symbol) [0x0x7ff771b27f9c]\n",
      "\t(No symbol) [0x0x7ff771b7ba87]\n",
      "\t(No symbol) [0x0x7ff771b503bf]\n",
      "\t(No symbol) [0x0x7ff771b787fb]\n",
      "\t(No symbol) [0x0x7ff771b50153]\n",
      "\t(No symbol) [0x0x7ff771b18b02]\n",
      "\t(No symbol) [0x0x7ff771b198d3]\n",
      "\tGetHandleVerifier [0x0x7ff77200e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff772008c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff7720286c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff771d6cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff771d750af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff771d5af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff771d5b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff771d410a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffe919de8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffe933a8d9c+44]\n",
      "\n",
      "   x failed all attempts: https://jobs.careers.microsoft.com/global/en/job/1882915/\n",
      "[71/408] SKIP already saved: 1879804\n",
      "[72/408] SKIP already saved: 1882970\n",
      "[73/408] SKIP already saved: 1835135\n",
      "[74/408] SKIP already saved: 1828317\n",
      "[75/408] SKIP already saved: 1857079\n",
      "[76/408] SKIP already saved: 1880869\n",
      "[77/408] SKIP already saved: 1882721\n",
      "[78/408] SKIP already saved: 1877196\n",
      "[79/408] SKIP already saved: 1881259\n",
      "[80/408] SKIP already saved: 1881132\n",
      "[81/408] SKIP already saved: 1882267\n",
      "[82/408] SKIP already saved: 1882817\n",
      "[83/408] SKIP already saved: 1881327\n",
      "[84/408] SKIP already saved: 1882132\n",
      "[85/408] SKIP already saved: 1852962\n",
      "[86/408] SKIP already saved: 1882081\n",
      "[87/408] SKIP already saved: 1835484\n",
      "[88/408] SKIP already saved: 1880104\n",
      "[89/408] SKIP already saved: 1881111\n",
      "[90/408] SKIP already saved: 1882620\n",
      "[91/408] SKIP already saved: 1878000\n",
      "[92/408] SKIP already saved: 1851179\n",
      "[93/408] SKIP already saved: 1872711\n",
      "[94/408] SKIP already saved: 1878977\n",
      "[95/408] SKIP already saved: 1881918\n",
      "[96/408] SKIP already saved: 1857752\n",
      "[97/408] SKIP already saved: 1881921\n",
      "[98/408] SKIP already saved: 1881824\n",
      "[99/408] SKIP already saved: 1879451\n",
      "[100/408] SKIP already saved: 1881100\n",
      "[101/408] SKIP already saved: 1881923\n",
      "[102/408] SKIP already saved: 1880983\n",
      "[103/408] GET https://jobs.careers.microsoft.com/global/en/job/1877503/\n",
      "   ! attempt 1 failed: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff771d51eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff771d51f10+80288]\n",
      "\t(No symbol) [0x0x7ff771ad02fa]\n",
      "\t(No symbol) [0x0x7ff771b27cd7]\n",
      "\t(No symbol) [0x0x7ff771b27f9c]\n",
      "\t(No symbol) [0x0x7ff771b7ba87]\n",
      "\t(No symbol) [0x0x7ff771b503bf]\n",
      "\t(No symbol) [0x0x7ff771b787fb]\n",
      "\t(No symbol) [0x0x7ff771b50153]\n",
      "\t(No symbol) [0x0x7ff771b18b02]\n",
      "\t(No symbol) [0x0x7ff771b198d3]\n",
      "\tGetHandleVerifier [0x0x7ff77200e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff772008c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff7720286c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff771d6cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff771d750af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff771d5af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff771d5b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff771d410a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffe919de8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffe933a8d9c+44]\n",
      "\n",
      "   ! attempt 2 failed: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff771d51eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff771d51f10+80288]\n",
      "\t(No symbol) [0x0x7ff771ad02fa]\n",
      "\t(No symbol) [0x0x7ff771b27cd7]\n",
      "\t(No symbol) [0x0x7ff771b27f9c]\n",
      "\t(No symbol) [0x0x7ff771b7ba87]\n",
      "\t(No symbol) [0x0x7ff771b503bf]\n",
      "\t(No symbol) [0x0x7ff771b787fb]\n",
      "\t(No symbol) [0x0x7ff771b50153]\n",
      "\t(No symbol) [0x0x7ff771b18b02]\n",
      "\t(No symbol) [0x0x7ff771b198d3]\n",
      "\tGetHandleVerifier [0x0x7ff77200e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff772008c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff7720286c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff771d6cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff771d750af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff771d5af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff771d5b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff771d410a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffe919de8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffe933a8d9c+44]\n",
      "\n",
      "   x failed all attempts: https://jobs.careers.microsoft.com/global/en/job/1877503/\n",
      "[104/408] SKIP already saved: 1877249\n",
      "[105/408] SKIP already saved: 1877519\n",
      "[106/408] SKIP already saved: 1879680\n",
      "[107/408] SKIP already saved: 1882206\n",
      "[108/408] SKIP already saved: 1882286\n",
      "[109/408] SKIP already saved: 1881253\n",
      "[110/408] SKIP already saved: 1874705\n",
      "[111/408] SKIP already saved: 1862283\n",
      "[112/408] GET https://jobs.careers.microsoft.com/global/en/job/1881265/\n",
      "   ! attempt 1 failed: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff771d51eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff771d51f10+80288]\n",
      "\t(No symbol) [0x0x7ff771ad02fa]\n",
      "\t(No symbol) [0x0x7ff771b27cd7]\n",
      "\t(No symbol) [0x0x7ff771b27f9c]\n",
      "\t(No symbol) [0x0x7ff771b7ba87]\n",
      "\t(No symbol) [0x0x7ff771b503bf]\n",
      "\t(No symbol) [0x0x7ff771b787fb]\n",
      "\t(No symbol) [0x0x7ff771b50153]\n",
      "\t(No symbol) [0x0x7ff771b18b02]\n",
      "\t(No symbol) [0x0x7ff771b198d3]\n",
      "\tGetHandleVerifier [0x0x7ff77200e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff772008c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff7720286c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff771d6cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff771d750af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff771d5af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff771d5b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff771d410a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffe919de8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffe933a8d9c+44]\n",
      "\n",
      "   ! attempt 2 failed: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff771d51eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff771d51f10+80288]\n",
      "\t(No symbol) [0x0x7ff771ad02fa]\n",
      "\t(No symbol) [0x0x7ff771b27cd7]\n",
      "\t(No symbol) [0x0x7ff771b27f9c]\n",
      "\t(No symbol) [0x0x7ff771b7ba87]\n",
      "\t(No symbol) [0x0x7ff771b503bf]\n",
      "\t(No symbol) [0x0x7ff771b787fb]\n",
      "\t(No symbol) [0x0x7ff771b50153]\n",
      "\t(No symbol) [0x0x7ff771b18b02]\n",
      "\t(No symbol) [0x0x7ff771b198d3]\n",
      "\tGetHandleVerifier [0x0x7ff77200e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff772008c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff7720286c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff771d6cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff771d750af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff771d5af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff771d5b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff771d410a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffe919de8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffe933a8d9c+44]\n",
      "\n",
      "   x failed all attempts: https://jobs.careers.microsoft.com/global/en/job/1881265/\n",
      "[113/408] SKIP already saved: 1881818\n",
      "[114/408] SKIP already saved: 1882244\n",
      "[115/408] SKIP already saved: 1882147\n",
      "[116/408] SKIP already saved: 1882212\n",
      "[117/408] SKIP already saved: 1880608\n",
      "[118/408] SKIP already saved: 1855521\n",
      "[119/408] SKIP already saved: 1855519\n",
      "[120/408] SKIP already saved: 1861364\n",
      "[121/408] SKIP already saved: 1879351\n",
      "[122/408] SKIP already saved: 1881919\n",
      "[123/408] SKIP already saved: 1880968\n",
      "[124/408] SKIP already saved: 1869160\n",
      "[125/408] SKIP already saved: 1876570\n",
      "[126/408] SKIP already saved: 1878947\n",
      "[127/408] SKIP already saved: 1880606\n",
      "[128/408] SKIP already saved: 1879811\n",
      "[129/408] SKIP already saved: 1849373\n",
      "[130/408] SKIP already saved: 1843250\n",
      "[131/408] SKIP already saved: 1881676\n",
      "[132/408] SKIP already saved: 1861591\n",
      "[133/408] SKIP already saved: 1882154\n",
      "[134/408] SKIP already saved: 1882694\n",
      "[135/408] SKIP already saved: 1881764\n",
      "[136/408] SKIP already saved: 1877695\n",
      "[137/408] SKIP already saved: 1880540\n",
      "[138/408] SKIP already saved: 1882655\n",
      "[139/408] SKIP already saved: 1880670\n",
      "[140/408] SKIP already saved: 1880656\n",
      "[141/408] SKIP already saved: 1880567\n",
      "[142/408] SKIP already saved: 1881106\n",
      "[143/408] SKIP already saved: 1857312\n",
      "[144/408] SKIP already saved: 1882142\n",
      "[145/408] SKIP already saved: 1875813\n",
      "[146/408] SKIP already saved: 1863861\n",
      "[147/408] SKIP already saved: 1880451\n",
      "[148/408] SKIP already saved: 1879430\n",
      "[149/408] SKIP already saved: 1881044\n",
      "[150/408] SKIP already saved: 1874255\n",
      "[151/408] SKIP already saved: 1880526\n",
      "[152/408] SKIP already saved: 1876873\n",
      "[153/408] SKIP already saved: 1878859\n",
      "[154/408] SKIP already saved: 1878848\n",
      "[155/408] SKIP already saved: 1879685\n",
      "[156/408] SKIP already saved: 1881865\n",
      "[157/408] SKIP already saved: 1878008\n",
      "[158/408] SKIP already saved: 1881053\n",
      "[159/408] SKIP already saved: 1861970\n",
      "[160/408] SKIP already saved: 1879796\n",
      "[161/408] SKIP already saved: 1875963\n",
      "[162/408] SKIP already saved: 1846716\n",
      "[163/408] SKIP already saved: 1881134\n",
      "[164/408] SKIP already saved: 1879064\n",
      "[165/408] SKIP already saved: 1878810\n",
      "[166/408] SKIP already saved: 1878809\n",
      "[167/408] SKIP already saved: 1882256\n",
      "[168/408] SKIP already saved: 1881950\n",
      "[169/408] SKIP already saved: 1854695\n",
      "[170/408] SKIP already saved: 1881869\n",
      "[171/408] SKIP already saved: 1876864\n",
      "[172/408] SKIP already saved: 1873018\n",
      "[173/408] SKIP already saved: 1861955\n",
      "[174/408] SKIP already saved: 1879533\n",
      "[175/408] SKIP already saved: 1880598\n",
      "[176/408] SKIP already saved: 1882156\n",
      "[177/408] SKIP already saved: 1881332\n",
      "[178/408] SKIP already saved: 1857812\n",
      "[179/408] SKIP already saved: 1881216\n",
      "[180/408] SKIP already saved: 1881241\n",
      "[181/408] SKIP already saved: 1858698\n",
      "[182/408] SKIP already saved: 1881313\n",
      "[183/408] SKIP already saved: 1875800\n",
      "[184/408] SKIP already saved: 1879700\n",
      "[185/408] SKIP already saved: 1875563\n",
      "[186/408] SKIP already saved: 1878975\n",
      "[187/408] SKIP already saved: 1878035\n",
      "[188/408] SKIP already saved: 1881276\n",
      "[189/408] SKIP already saved: 1828770\n",
      "[190/408] SKIP already saved: 1879713\n",
      "[191/408] SKIP already saved: 1876026\n",
      "[192/408] SKIP already saved: 1857039\n",
      "[193/408] SKIP already saved: 1882198\n",
      "[194/408] SKIP already saved: 1864645\n",
      "[195/408] SKIP already saved: 1882171\n",
      "[196/408] SKIP already saved: 1836776\n",
      "[197/408] SKIP already saved: 1882176\n",
      "[198/408] SKIP already saved: 1877646\n",
      "[199/408] SKIP already saved: 1881942\n",
      "[200/408] SKIP already saved: 1836064\n",
      "[201/408] SKIP already saved: 1881295\n",
      "[202/408] SKIP already saved: 1884077\n",
      "[203/408] SKIP already saved: 1880539\n",
      "[204/408] SKIP already saved: 1882737\n",
      "[205/408] SKIP already saved: 1859937\n",
      "[206/408] SKIP already saved: 1882250\n",
      "[207/408] SKIP already saved: 1874243\n",
      "[208/408] SKIP already saved: 1864545\n",
      "[209/408] SKIP already saved: 1884039\n",
      "[210/408] SKIP already saved: 1881926\n",
      "[211/408] SKIP already saved: 1856745\n",
      "[212/408] SKIP already saved: 1874914\n",
      "[213/408] SKIP already saved: 1875541\n",
      "[214/408] SKIP already saved: 1882251\n",
      "[215/408] SKIP already saved: 1851485\n",
      "[216/408] SKIP already saved: 1845905\n",
      "[217/408] SKIP already saved: 1883582\n",
      "[218/408] SKIP already saved: 1859554\n",
      "[219/408] SKIP already saved: 1826999\n",
      "[220/408] SKIP already saved: 1882243\n",
      "[221/408] SKIP already saved: 1847433\n",
      "[222/408] SKIP already saved: 1883975\n",
      "[223/408] SKIP already saved: 1881808\n",
      "[224/408] SKIP already saved: 1851573\n",
      "[225/408] SKIP already saved: 1883974\n",
      "[226/408] SKIP already saved: 1881303\n",
      "[227/408] SKIP already saved: 1874870\n",
      "[228/408] SKIP already saved: 1882684\n",
      "[229/408] SKIP already saved: 1882231\n",
      "[230/408] SKIP already saved: 1882214\n",
      "[231/408] SKIP already saved: 1836060\n",
      "[232/408] SKIP already saved: 1861611\n",
      "[233/408] SKIP already saved: 1880471\n",
      "[234/408] SKIP already saved: 1883845\n",
      "[235/408] SKIP already saved: 1841354\n",
      "[236/408] SKIP already saved: 1841348\n",
      "[237/408] SKIP already saved: 1878957\n",
      "[238/408] SKIP already saved: 1868426\n",
      "[239/408] SKIP already saved: 1883880\n",
      "[240/408] SKIP already saved: 1882126\n",
      "[241/408] SKIP already saved: 1883321\n",
      "[242/408] SKIP already saved: 1849013\n",
      "[243/408] SKIP already saved: 1874276\n",
      "[244/408] SKIP already saved: 1881783\n",
      "[245/408] SKIP already saved: 1881153\n",
      "[246/408] SKIP already saved: 1883863\n",
      "[247/408] SKIP already saved: 1881728\n",
      "[248/408] SKIP already saved: 1853077\n",
      "[249/408] SKIP already saved: 1836960\n",
      "[250/408] SKIP already saved: 1880576\n",
      "[251/408] SKIP already saved: 1879217\n",
      "[252/408] SKIP already saved: 1880533\n",
      "[253/408] SKIP already saved: 1863900\n",
      "[254/408] SKIP already saved: 1830570\n",
      "[255/408] SKIP already saved: 1844809\n",
      "[256/408] SKIP already saved: 1881032\n",
      "[257/408] SKIP already saved: 1826987\n",
      "[258/408] SKIP already saved: 1883711\n",
      "[259/408] SKIP already saved: 1878955\n",
      "[260/408] SKIP already saved: 1883643\n",
      "[261/408] SKIP already saved: 1874727\n",
      "[262/408] SKIP already saved: 1880675\n",
      "[263/408] SKIP already saved: 1858569\n",
      "[264/408] SKIP already saved: 1882291\n",
      "[265/408] SKIP already saved: 1882740\n",
      "[266/408] SKIP already saved: 1882764\n",
      "[267/408] SKIP already saved: 1873070\n",
      "[268/408] SKIP already saved: 1881772\n",
      "[269/408] SKIP already saved: 1840248\n",
      "[270/408] SKIP already saved: 1865074\n",
      "[271/408] SKIP already saved: 1883284\n",
      "[272/408] SKIP already saved: 1876890\n",
      "[273/408] SKIP already saved: 1863542\n",
      "[274/408] SKIP already saved: 1882269\n",
      "[275/408] SKIP already saved: 1823366\n",
      "[276/408] SKIP already saved: 1882648\n",
      "[277/408] SKIP already saved: 1881836\n",
      "[278/408] SKIP already saved: 1880481\n",
      "[279/408] SKIP already saved: 1878119\n",
      "[280/408] SKIP already saved: 1881356\n",
      "[281/408] SKIP already saved: 1883523\n",
      "[282/408] SKIP already saved: 1879091\n",
      "[283/408] SKIP already saved: 1882182\n",
      "[284/408] SKIP already saved: 1881362\n",
      "[285/408] SKIP already saved: 1856943\n",
      "[286/408] SKIP already saved: 1872506\n",
      "[287/408] SKIP already saved: 1875959\n",
      "[288/408] SKIP already saved: 1881848\n",
      "[289/408] SKIP already saved: 1855384\n",
      "[290/408] SKIP already saved: 1841252\n",
      "[291/408] SKIP already saved: 1882165\n",
      "[292/408] SKIP already saved: 1876780\n",
      "[293/408] SKIP already saved: 1882314\n",
      "[294/408] SKIP already saved: 1881912\n",
      "[295/408] SKIP already saved: 1882761\n",
      "[296/408] SKIP already saved: 1876585\n",
      "[297/408] SKIP already saved: 1846112\n",
      "[298/408] SKIP already saved: 1863532\n",
      "[299/408] SKIP already saved: 1865076\n",
      "[300/408] SKIP already saved: 1883645\n",
      "[301/408] SKIP already saved: 1880529\n",
      "[302/408] SKIP already saved: 1882431\n",
      "[303/408] SKIP already saved: 1858310\n",
      "[304/408] SKIP already saved: 1884194\n",
      "[305/408] SKIP already saved: 1884612\n",
      "[306/408] SKIP already saved: 1842661\n",
      "[307/408] SKIP already saved: 1882878\n",
      "[308/408] SKIP already saved: 1884156\n",
      "[309/408] SKIP already saved: 1879056\n",
      "[310/408] SKIP already saved: 1884153\n",
      "[311/408] SKIP already saved: 1884146\n",
      "[312/408] SKIP already saved: 1864732\n",
      "[313/408] SKIP already saved: 1875535\n",
      "[314/408] SKIP already saved: 1884141\n",
      "[315/408] SKIP already saved: 1884120\n",
      "[316/408] SKIP already saved: 1848031\n",
      "[317/408] SKIP already saved: 1880594\n",
      "[318/408] SKIP already saved: 1819624\n",
      "[319/408] SKIP already saved: 1833803\n",
      "[320/408] SKIP already saved: 1882646\n",
      "[321/408] SKIP already saved: 1883691\n",
      "[322/408] SKIP already saved: 1882830\n",
      "[323/408] SKIP already saved: 1875607\n",
      "[324/408] SKIP already saved: 1872918\n",
      "[325/408] SKIP already saved: 1870870\n",
      "[326/408] SKIP already saved: 1842582\n",
      "[327/408] SKIP already saved: 1879563\n",
      "[328/408] SKIP already saved: 1861505\n",
      "[329/408] SKIP already saved: 1861552\n",
      "[330/408] SKIP already saved: 1861522\n",
      "[331/408] SKIP already saved: 1852186\n",
      "[332/408] SKIP already saved: 1846809\n",
      "[333/408] SKIP already saved: 1882697\n",
      "[334/408] SKIP already saved: 1882860\n",
      "[335/408] SKIP already saved: 1882259\n",
      "[336/408] SKIP already saved: 1881856\n",
      "[337/408] SKIP already saved: 1839980\n",
      "[338/408] SKIP already saved: 1879165\n",
      "[339/408] SKIP already saved: 1882923\n",
      "[340/408] SKIP already saved: 1881989\n",
      "[341/408] SKIP already saved: 1883207\n",
      "[342/408] SKIP already saved: 1880525\n",
      "[343/408] SKIP already saved: 1860395\n",
      "[344/408] SKIP already saved: 1881683\n",
      "[345/408] SKIP already saved: 1882667\n",
      "[346/408] SKIP already saved: 1883497\n",
      "[347/408] SKIP already saved: 1884223\n",
      "[348/408] SKIP already saved: 1884232\n",
      "[349/408] SKIP already saved: 1882753\n",
      "[350/408] SKIP already saved: 1874039\n",
      "[351/408] SKIP already saved: 1884231\n",
      "[352/408] SKIP already saved: 1848226\n",
      "[353/408] SKIP already saved: 1881575\n",
      "[354/408] SKIP already saved: 1884237\n",
      "[355/408] SKIP already saved: 1860463\n",
      "[356/408] SKIP already saved: 1884229\n",
      "[357/408] SKIP already saved: 1883419\n",
      "[358/408] SKIP already saved: 1875817\n",
      "[359/408] SKIP already saved: 1862377\n",
      "[360/408] SKIP already saved: 1881667\n",
      "[361/408] SKIP already saved: 1836063\n",
      "[362/408] SKIP already saved: 1875861\n",
      "[363/408] SKIP already saved: 1874005\n",
      "[364/408] SKIP already saved: 1823064\n",
      "[365/408] SKIP already saved: 1879633\n",
      "[366/408] SKIP already saved: 1875736\n",
      "[367/408] SKIP already saved: 1875049\n",
      "[368/408] SKIP already saved: 1882262\n",
      "[369/408] SKIP already saved: 1882748\n",
      "[370/408] SKIP already saved: 1852560\n",
      "[371/408] SKIP already saved: 1881189\n",
      "[372/408] SKIP already saved: 1861360\n",
      "[373/408] SKIP already saved: 1883639\n",
      "[374/408] SKIP already saved: 1880994\n",
      "[375/408] SKIP already saved: 1884111\n",
      "[376/408] SKIP already saved: 1872535\n",
      "[377/408] SKIP already saved: 1864627\n",
      "[378/408] SKIP already saved: 1876623\n",
      "[379/408] SKIP already saved: 1881929\n",
      "[380/408] SKIP already saved: 1878007\n",
      "[381/408] SKIP already saved: 1875638\n",
      "[382/408] SKIP already saved: 1877934\n",
      "[383/408] SKIP already saved: 1883548\n",
      "[384/408] SKIP already saved: 1884152\n",
      "[385/408] SKIP already saved: 1883557\n",
      "[386/408] SKIP already saved: 1879174\n",
      "[387/408] SKIP already saved: 1862412\n",
      "[388/408] SKIP already saved: 1860821\n",
      "[389/408] GET https://jobs.careers.microsoft.com/global/en/job/1863994/\n",
      "[390/408] GET https://jobs.careers.microsoft.com/global/en/job/1872492/\n",
      "[391/408] GET https://jobs.careers.microsoft.com/global/en/job/1876065/\n",
      "[392/408] GET https://jobs.careers.microsoft.com/global/en/job/1879466/\n",
      "[393/408] GET https://jobs.careers.microsoft.com/global/en/job/1864253/\n",
      "   - checkpoint saved (5 records)\n",
      "[394/408] GET https://jobs.careers.microsoft.com/global/en/job/1862382/\n",
      "[395/408] GET https://jobs.careers.microsoft.com/global/en/job/1842932/\n",
      "[396/408] GET https://jobs.careers.microsoft.com/global/en/job/1882263/\n",
      "[397/408] GET https://jobs.careers.microsoft.com/global/en/job/1877977/\n",
      "[398/408] GET https://jobs.careers.microsoft.com/global/en/job/1884196/\n",
      "   - checkpoint saved (10 records)\n",
      "   - restarting browser after 10 jobs\n",
      "[399/408] GET https://jobs.careers.microsoft.com/global/en/job/1882617/\n",
      "[400/408] GET https://jobs.careers.microsoft.com/global/en/job/1881945/\n",
      "[401/408] GET https://jobs.careers.microsoft.com/global/en/job/1879860/\n",
      "[402/408] GET https://jobs.careers.microsoft.com/global/en/job/1882896/\n",
      "[403/408] GET https://jobs.careers.microsoft.com/global/en/job/1882564/\n",
      "   - checkpoint saved (15 records)\n",
      "[404/408] GET https://jobs.careers.microsoft.com/global/en/job/1868409/\n",
      "[405/408] GET https://jobs.careers.microsoft.com/global/en/job/1885079/\n",
      "[406/408] GET https://jobs.careers.microsoft.com/global/en/job/1868379/\n",
      "[407/408] GET https://jobs.careers.microsoft.com/global/en/job/1881141/\n",
      "[408/408] GET https://jobs.careers.microsoft.com/global/en/job/1880524/\n",
      "   - checkpoint saved (20 records)\n",
      "[DONE] wrote 405 records to jobs_ms_details.json\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade selenium beautifulsoup4\n",
    "\n",
    "import os, re, json, time\n",
    "from typing import Dict, Any, List\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "\n",
    "DB_PATH_IN  = \"jobs_ms.json\"           # your existing list/index (from the search pages)\n",
    "DB_PATH_OUT = \"jobs_ms_details.json\"   # NEW details DB (this script writes here)\n",
    "\n",
    "LABELS = [\n",
    "    \"Date posted\",\"Work site\",\"Role type\",\"Discipline\",\n",
    "    \"Job number\",\"Travel\",\"Profession\",\"Employment type\"\n",
    "]\n",
    "\n",
    "# polite delay between requests (seconds)\n",
    "SLEEP_BETWEEN = (0.6, 1.2)  # (min, max)\n",
    "MAX_RETRIES   = 2           # retries per job page on failures\n",
    "\n",
    "\n",
    "# -------------------- UTILS --------------------\n",
    "\n",
    "def norm(s: str | None) -> str:\n",
    "    import re as _re\n",
    "    return _re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def sleep_a_bit():\n",
    "    import random\n",
    "    time.sleep(random.uniform(*SLEEP_BETWEEN))\n",
    "\n",
    "def load_jobs_index(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load jobs_ms.json -> return a list of job rows with url or job_id.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    rows = list(data.values()) if isinstance(data, dict) else list(data)\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"jobs_ms.json has no rows\")\n",
    "    return rows\n",
    "\n",
    "def load_details_db(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path): return {}\n",
    "    try:\n",
    "        return json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def save_details_db(db: Dict[str, Any], path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(db, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def make_job_url(row: Dict[str, Any]) -> str | None:\n",
    "    if row.get(\"url\"):\n",
    "        return row[\"url\"]\n",
    "    jid = row.get(\"job_id\")\n",
    "    if jid:\n",
    "        return f\"https://jobs.careers.microsoft.com/global/en/job/{jid}/\"\n",
    "    return None\n",
    "\n",
    "def upsert(rec: Dict[str, Any], db: Dict[str, Any]) -> None:\n",
    "    key = str(rec.get(\"job_id\") or rec.get(\"url\"))\n",
    "    if not key:\n",
    "        return\n",
    "    if key not in db:\n",
    "        db[key] = rec\n",
    "    else:\n",
    "        # update only non-empty values\n",
    "        for k, v in rec.items():\n",
    "            if v not in (None, \"\", []):\n",
    "                db[key][k] = v\n",
    "\n",
    "\n",
    "# -------------------- PAY / LOC HELPERS --------------------\n",
    "\n",
    "USD_RANGE  = re.compile(r\"USD\\s*\\$\\s*[\\d,]+\\s*-\\s*\\$\\s*[\\d,]+\", re.I)\n",
    "PAY_START  = re.compile(\n",
    "    r\"(typical\\s+base\\s+pay\\s+range|base\\s+pay\\s+range\\s+for\\s+this\\s+role|benefits\\s+and\\s+pay\\s+information|USD\\s*\\$\\s*[\\d,]+\\s*-\\s*\\$\\s*[\\d,]+)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def extract_pay_ranges(text: str) -> List[Dict[str,str]]:\n",
    "    spans = []\n",
    "    for m in USD_RANGE.finditer(text):\n",
    "        s, e = m.span()\n",
    "        ctx = text[max(0, s-140): min(len(text), e+140)]\n",
    "        region = \"U.S.\"\n",
    "        if re.search(r\"San\\s*Francisco\\s*Bay|New\\s*York\\s*City\", ctx, re.I):\n",
    "            region = \"SF Bay Area / NYC\"\n",
    "        spans.append({\"region\": region, \"range\": m.group(0)})\n",
    "    # de-dup\n",
    "    uniq, seen = [], set()\n",
    "    for r in spans:\n",
    "        key = (r[\"region\"], r[\"range\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key); uniq.append(r)\n",
    "    return uniq\n",
    "\n",
    "def extract_locations_jsonld(html_text: str) -> List[str]:\n",
    "    out = []\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    for tag in soup.find_all(\"script\", {\"type\": \"application/ld+json\"}):\n",
    "        try:\n",
    "            data = json.loads(tag.string or \"\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        items = data if isinstance(data, list) else [data]\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and it.get(\"@type\") in {\"JobPosting\",\"Posting\"}:\n",
    "                jl = it.get(\"jobLocation\")\n",
    "                if isinstance(jl, dict): jl = [jl]\n",
    "                if isinstance(jl, list):\n",
    "                    for loc in jl:\n",
    "                        addr = (loc or {}).get(\"address\", {})\n",
    "                        parts = [addr.get(\"addressLocality\"), addr.get(\"addressRegion\"), addr.get(\"addressCountry\")]\n",
    "                        parts = [p for p in parts if p]\n",
    "                        if parts: out.append(\", \".join(parts))\n",
    "    # de-dup\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "\n",
    "# -------------------- QUALIFICATIONS SPLIT (YOUR RULES) --------------------\n",
    "\n",
    "REQ_RE   = re.compile(r\"\\bRequired\\s+Qualifications\\b\", re.I)\n",
    "PREF_RE  = re.compile(r\"\\bPreferred\\s+Qualifications\\b\", re.I)\n",
    "OTHER_RE = re.compile(r\"\\bOther\\s+Requirements?\\b\", re.I)\n",
    "\n",
    "def block_text_from_html(html: str) -> str:\n",
    "    \"\"\"Convert block-level HTML into newline-separated text preserving bullets.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    pieces = []\n",
    "    for node in soup.descendants:\n",
    "        if isinstance(node, NavigableString):  # we'll take text from elements instead\n",
    "            continue\n",
    "        if node.name in (\"ul\",\"ol\"):\n",
    "            for li in node.select(\":scope > li\"):\n",
    "                t = norm(li.get_text(\" \", strip=True))\n",
    "                if t: pieces.append(\"• \" + t)\n",
    "        elif node.name in (\"p\",\"div\",\"section\"):\n",
    "            t = norm(node.get_text(\" \", strip=True))\n",
    "            if t: pieces.append(t)\n",
    "    # collapse duplicate consecutive lines\n",
    "    out = []\n",
    "    for p in pieces:\n",
    "        if not out or p != out[-1]:\n",
    "            out.append(p)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def find_span(text: str, pattern: re.Pattern, start_at: int = 0):\n",
    "    m = pattern.search(text, start_at)\n",
    "    return (m.start(), m.end()) if m else (None, None)\n",
    "\n",
    "def slice_between(text: str, start_pat: re.Pattern, end_pats: tuple[re.Pattern, ...], start_offset_to_content=True) -> str:\n",
    "    s0, s1 = find_span(text, start_pat)\n",
    "    if s0 is None: return \"\"\n",
    "    start = s1 if start_offset_to_content else s0\n",
    "    ends = []\n",
    "    for ep in end_pats:\n",
    "        e0, _ = find_span(text, ep, start_at=start)\n",
    "        if e0 is not None:\n",
    "            ends.append(e0)\n",
    "    stop = min(ends) if ends else len(text)\n",
    "    return text[start:stop].strip()\n",
    "\n",
    "def split_qualifications(qual_text: str):\n",
    "    \"\"\"\n",
    "    Rules:\n",
    "      - Required: from 'Required Qualifications' up to (Other OR Preferred)\n",
    "      - Other   : from 'Other Requirements' up to 'Preferred Qualifications'\n",
    "      - Preferred: from 'Preferred Qualifications' up to the pay paragraph\n",
    "    \"\"\"\n",
    "    q = qual_text\n",
    "    pay_start_idx, _ = find_span(q, PAY_START)\n",
    "    pay_enders = (PAY_START,) if pay_start_idx is not None else ()\n",
    "\n",
    "    required_text  = slice_between(q, REQ_RE,   (OTHER_RE, PREF_RE))\n",
    "    other_text     = slice_between(q, OTHER_RE, (PREF_RE,))\n",
    "    preferred_text = slice_between(q, PREF_RE,  pay_enders)\n",
    "\n",
    "    # fallbacks if markers exist but end wasn't found\n",
    "    if not required_text and REQ_RE.search(q):\n",
    "        required_text = slice_between(q, REQ_RE, pay_enders)\n",
    "    if not other_text and OTHER_RE.search(q):\n",
    "        other_text = slice_between(q, OTHER_RE, pay_enders)\n",
    "    if not preferred_text and PREF_RE.search(q):\n",
    "        preferred_text = slice_between(q, PREF_RE, ())\n",
    "\n",
    "    return required_text, preferred_text, other_text\n",
    "\n",
    "\n",
    "# -------------------- SELENIUM SCRAPER --------------------\n",
    "\n",
    "def launch_chrome():\n",
    "    opt = ChromeOptions()\n",
    "    opt.add_argument(\"--headless=new\")\n",
    "    opt.add_argument(\"--no-sandbox\")\n",
    "    opt.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opt.add_argument(\"--window-size=1400,2200\")\n",
    "    return webdriver.Chrome(options=opt)\n",
    "\n",
    "def safe_text(el) -> str | None:\n",
    "    try:\n",
    "        return norm(el.text)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_detail_page(url: str, driver: webdriver.Chrome) -> Dict[str, Any]:\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 35).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "    time.sleep(0.7)\n",
    "\n",
    "    # anchor by \"Date posted\" and find the title just before it\n",
    "    dp = WebDriverWait(driver, 25).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[normalize-space()='Date posted']\"))\n",
    "    )\n",
    "    title_el = dp.find_element(By.XPATH, \"preceding::h1[1]\")\n",
    "    title = safe_text(title_el)\n",
    "\n",
    "    # panel = smallest ancestor containing both title & the \"Date posted\" label\n",
    "    panel = title_el.find_element(By.XPATH, \"ancestor::*[.//*[normalize-space()='Date posted']][1]\")\n",
    "\n",
    "    # location: first meaningful text after <h1> inside the panel (skips Apply/Save area)\n",
    "    location = None\n",
    "    try:\n",
    "        cand = panel.find_element(By.XPATH, \".//h1/following::*[normalize-space()][1]\")\n",
    "        txt = safe_text(cand)\n",
    "        if txt and not any(x in txt for x in (\"Apply\", \"Save\", \"Share job\")):\n",
    "            location = txt\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # key/value fields\n",
    "    def value_for(label: str) -> str | None:\n",
    "        try:\n",
    "            lab = panel.find_element(By.XPATH, f\".//*[normalize-space()='{label}' or normalize-space()='{label}:']\")\n",
    "        except Exception:\n",
    "            return None\n",
    "        for rel in [\"./following-sibling::*[normalize-space()][1]\",\n",
    "                    \"following::*[normalize-space()][1]\"]:\n",
    "            try:\n",
    "                node = lab.find_element(By.XPATH, rel)\n",
    "                val = safe_text(node)\n",
    "                if val: return val\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None\n",
    "\n",
    "    fields = {lab: value_for(lab) for lab in LABELS}\n",
    "\n",
    "    # Qualifications HTML (block after its heading)\n",
    "    q_html = \"\"\n",
    "    try:\n",
    "        qh = panel.find_element(By.XPATH, \".//h2[normalize-space()='Qualifications'] | .//h3[normalize-space()='Qualifications']\")\n",
    "        frag, sib = [], qh\n",
    "        for _ in range(160):\n",
    "            try:\n",
    "                sib = sib.find_element(By.XPATH, \"following-sibling::*[1]\")\n",
    "            except Exception:\n",
    "                break\n",
    "            tg = sib.tag_name.lower()\n",
    "            if tg in (\"h2\",\"h3\"): break\n",
    "            frag.append(sib.get_attribute(\"outerHTML\"))\n",
    "        q_html = \"\".join(frag)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    qualifications_text = block_text_from_html(q_html) if q_html else \"\"\n",
    "    req_text, pref_text, other_text = split_qualifications(qualifications_text)\n",
    "\n",
    "    # pay ranges from qualifications (Microsoft places pay block there)\n",
    "    pay_ranges = extract_pay_ranges(qualifications_text)\n",
    "\n",
    "    # locations fallback via JSON-LD if needed\n",
    "    if not location:\n",
    "        jl = extract_locations_jsonld(driver.page_source)\n",
    "        if jl: location = \" | \".join(jl)\n",
    "\n",
    "    # job id\n",
    "    m = re.search(r\"/job/(\\d+)\", driver.current_url)\n",
    "    job_id = fields.get(\"Job number\") or (m.group(1) if m else None)\n",
    "\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"title\": title,\n",
    "        \"url\": driver.current_url,\n",
    "        \"date_posted\": fields.get(\"Date posted\"),\n",
    "        \"locations\": [location] if location else [],\n",
    "        \"travel\": fields.get(\"Travel\"),\n",
    "        # FULL TEXT blocks (as requested)\n",
    "        \"required_qualifications_text\": req_text,\n",
    "        \"other_requirements_text\":     other_text,\n",
    "        \"preferred_qualifications_text\": pref_text,\n",
    "        \"qualifications_text\": qualifications_text,   # entire section for reference\n",
    "        \"pay_ranges\": pay_ranges,\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- MAIN: PROCESS ALL URLS --------------------\n",
    "\n",
    "def main():\n",
    "    rows = load_jobs_index(DB_PATH_IN)\n",
    "    \n",
    "    # Build URLs list...\n",
    "    urls = []\n",
    "    seen = set()\n",
    "    for row in rows:\n",
    "        url = make_job_url(row)\n",
    "        if not url: continue\n",
    "        if url not in seen:\n",
    "            seen.add(url)\n",
    "            urls.append(url)\n",
    "\n",
    "    details_db = load_details_db(DB_PATH_OUT)\n",
    "    \n",
    "    # Restart browser every 10 jobs to prevent memory issues\n",
    "    RESTART_EVERY = 10\n",
    "    drv = None\n",
    "    processed = 0\n",
    "    \n",
    "    try:\n",
    "        print(f\"[DETAILS] processing {len(urls)} job pages …\")\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            key = re.search(r\"/job/(\\d+)\", url)\n",
    "            key = key.group(1) if key else url\n",
    "\n",
    "            if key in details_db:\n",
    "                print(f\"[{i}/{len(urls)}] SKIP already saved: {key}\")\n",
    "                continue\n",
    "\n",
    "            # Restart browser every RESTART_EVERY jobs\n",
    "            if drv is None or (processed > 0 and processed % RESTART_EVERY == 0):\n",
    "                if drv:\n",
    "                    print(f\"   - restarting browser after {RESTART_EVERY} jobs\")\n",
    "                    drv.quit()\n",
    "                    time.sleep(2)\n",
    "                drv = launch_chrome()\n",
    "\n",
    "            print(f\"[{i}/{len(urls)}] GET {url}\")\n",
    "            success = False\n",
    "            for attempt in range(1, MAX_RETRIES+1):\n",
    "                try:\n",
    "                    rec = parse_detail_page(url, drv)\n",
    "                    upsert(rec, details_db)\n",
    "                    processed += 1\n",
    "                    success = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"   ! attempt {attempt} failed: {e}\")\n",
    "                    if \"chrome\" in str(e).lower() or \"session\" in str(e).lower():\n",
    "                        # Browser crash - restart it\n",
    "                        try:\n",
    "                            drv.quit()\n",
    "                        except:\n",
    "                            pass\n",
    "                        time.sleep(2)\n",
    "                        drv = launch_chrome()\n",
    "                    time.sleep(1.0)\n",
    "\n",
    "            if not success:\n",
    "                print(f\"   x failed all attempts: {url}\")\n",
    "\n",
    "            # Checkpoint save every 5 records\n",
    "            if processed and processed % 5 == 0:\n",
    "                save_details_db(details_db, DB_PATH_OUT)\n",
    "                print(f\"   - checkpoint saved ({processed} records)\")\n",
    "\n",
    "            sleep_a_bit()\n",
    "\n",
    "    finally:\n",
    "        save_details_db(details_db, DB_PATH_OUT)\n",
    "        if drv:\n",
    "            drv.quit()\n",
    "\n",
    "    print(f\"[DONE] wrote {len(details_db)} records to {DB_PATH_OUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750daa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] scanned 405 jobs from jobs_ms_details.json\n",
      "[OK] wrote ONLY hits to jobs_ms_avoid_hits_by_field.json\n",
      "  - clearance_required: 12 job(s)\n",
      "  - knowledge_fullstack: 25 job(s)\n",
      "  - knowledge_python: 169 job(s)\n",
      "  - senior_only: 60 job(s)\n",
      "  - unwanted_languages: 133 job(s)\n",
      "  - unwanted_positions: 67 job(s)\n",
      "  - visa_sponsorship_block: 1 job(s)\n"
     ]
    }
   ],
   "source": [
    "# run with: python flag_jobs_by_field_minimal.py\n",
    "\n",
    "import os, re, json\n",
    "from typing import Dict, List, Any, Iterable\n",
    "\n",
    "DETAILS_PATH = \"jobs_ms_details.json\"                 # input (your detailed jobs)\n",
    "OUTPUT_PATH  = \"jobs_ms_avoid_hits_by_field.json\"     # output (only hits, no empties)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# EDIT: classes -> { field -> [keywords...] }\n",
    "# \"*\" applies to ALL scannable fields\n",
    "# fields should match keys in your details JSON\n",
    "# ----------------------------------------------\n",
    "AVOID_RULES: Dict[str, Dict[str, List[str]]] = {\n",
    "    \"visa_sponsorship_block\": {\n",
    "        \"title\": [\"no sponsorship\", \"no visa\"],\n",
    "        \"qualifications_text\": [\"without sponsorship\"],\n",
    "        \"other_requirements_text\": [\"citizens only\", \"citizenship required\", \"citizenship is required\", \"U.S. citizens\", \"US citizens\", \"green card\", \"permanent resident\"],\n",
    "    },\n",
    "    \"senior_only\": {\n",
    "        \"title\": [\"principal only\", \"senior only\"],\n",
    "        \"required_qualifications_text\": [\"6+ years\", \"10+ years\", \"12+ years\"],\n",
    "    },\n",
    "    \"clearance_required\": {\n",
    "        \"other_requirements_text\": [\"security clearance\", \"public trust\", \"polygraph\"],\n",
    "    },\n",
    "    \"knowledge_fullstack\": {\n",
    "        \"required_qualifications_text\": [\"HTML\", \"React\", \"Node.js\", \"REST\", \"Full Stack\", \"Full-Stack\", \"Fullstack\", \"Front End\", \"Frontend\", \"Back End\", \"Backend\", \"API\",\n",
    "                  \"Angular\", \"Vue.js\", \"Django\", \"Flask\", \"Ruby on Rails\", \"PHP\", \"http\", \"HTTP\", \"HTTPS\", \"https\"],\n",
    "    },\n",
    "    \"unwanted_languages\": {\n",
    "        \"required_qualifications_text\": [\"java\", \"javascript\", \"c#\", \"c-sharp\", \"c plus plus\", \"c++\", \"ruby\", \"php\", \"swift\", \"kotlin\", \"go \", \"golang\", \"r \", \"perl\", \"scala\", \"haskell\", \"lua\"],\n",
    "    },\n",
    "    \"knowledge_python\": {\n",
    "        \"*\": [\"python\"],\n",
    "    },\n",
    "    \"unwanted_positions\": {\n",
    "        \"title\": [\"finance\", \"accounting\", \"recruiter\", \"recruitment\", \n",
    "                  \"salesforce\", \"sales force\", \"sales\", \"marketing\", \n",
    "                  \"legal\", \"attorney\", \"lawyer\", \"paralegal\", \"compliance\",\n",
    "                  \"human resources\", \"hr \", \"talent acquisition\", \"talent management\",\n",
    "                  \"UX designer\", \"user experience\", \"graphic designer\", \"ui designer\",\n",
    "                  \"technical writer\", \"content writer\", \"copywriter\",],\n",
    "        \"required_qualifications_text\": [\"finance\", \"accounting\", \"recruiter\", \"recruitment\",\n",
    "                  \"salesforce\", \"sales force\", \"sales\", \"marketing\",\n",
    "                  \"legal\", \"attorney\", \"lawyer\", \"paralegal\", \"compliance\",\n",
    "                  \"human resources\", \"hr \", \"talent acquisition\", \"talent management\",\n",
    "                  \"UX designer\", \"user experience\", \"graphic designer\", \"ui designer\",\n",
    "                  \"technical writer\", \"content writer\", \"copywriter\",],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Limit which fields we scan (None = auto-detect string/list/dict fields)\n",
    "SCANNABLE_FIELDS: List[str] | None = [\n",
    "    \"title\",\n",
    "    \"locations\",\n",
    "    \"travel\",\n",
    "    \"qualifications_text\",\n",
    "    \"required_qualifications_text\",\n",
    "    \"preferred_qualifications_text\",\n",
    "    \"other_requirements_text\",\n",
    "    \"date_posted\",\n",
    "]\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "def norm(s: str | None) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def to_text(val: Any) -> str:\n",
    "    \"\"\"Normalize any field into plain text (lowercased) for matching.\"\"\"\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    if isinstance(val, list):\n",
    "        parts: List[str] = []\n",
    "        for x in val:\n",
    "            if isinstance(x, dict):\n",
    "                parts.append(json.dumps(x, ensure_ascii=False))\n",
    "            else:\n",
    "                parts.append(str(x))\n",
    "        return norm(\" | \".join(parts)).lower()\n",
    "    if isinstance(val, dict):\n",
    "        return norm(json.dumps(val, ensure_ascii=False)).lower()\n",
    "    return norm(str(val)).lower()\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj: Any, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def get_job_id(key: str, rec: Dict[str, Any]) -> str:\n",
    "    if rec.get(\"job_id\"):\n",
    "        return str(rec[\"job_id\"])\n",
    "    m = re.search(r\"/job/(\\d+)\", rec.get(\"url\") or key or \"\")\n",
    "    return m.group(1) if m else (rec.get(\"url\") or key or \"UNKNOWN\")\n",
    "\n",
    "def iter_scannable_fields(rec: Dict[str, Any]) -> Iterable[str]:\n",
    "    if SCANNABLE_FIELDS is not None:\n",
    "        for f in SCANNABLE_FIELDS:\n",
    "            if f in rec:\n",
    "                yield f\n",
    "    else:\n",
    "        for f, v in rec.items():\n",
    "            if isinstance(v, (str, list, dict)):\n",
    "                yield f\n",
    "\n",
    "def kw_boundary_search(blob: str, kw: str) -> bool:\n",
    "    \"\"\"Case-insensitive word-ish boundary search to avoid 'java' in 'javascript'.\"\"\"\n",
    "    return re.search(rf\"(?<!\\w){re.escape(kw)}(?!\\w)\", blob, re.I) is not None\n",
    "\n",
    "def materialize_field_keywords(per_field: Dict[str, List[str]], available_fields: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Merge wildcard '*' kws into each available field, plus its explicit kws.\n",
    "    Returns only fields that end up with >=1 keyword.\n",
    "    \"\"\"\n",
    "    result: Dict[str, List[str]] = {}\n",
    "    wild = per_field.get(\"*\", [])\n",
    "    for f in available_fields:\n",
    "        kws = set(wild)\n",
    "        if f in per_field:\n",
    "            kws.update(per_field[f])\n",
    "        if kws:\n",
    "            result[f] = sorted(kws)\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    details = load_json(DETAILS_PATH)  # dict: key -> record\n",
    "\n",
    "    # We'll only add classes that have at least one hit.\n",
    "    hits_out: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    total = 0\n",
    "    total_hits = 0\n",
    "\n",
    "    for key, rec in details.items():\n",
    "        total += 1\n",
    "        job_id = get_job_id(key, rec)\n",
    "\n",
    "        # cache blobs per field for this record\n",
    "        available_fields = list(iter_scannable_fields(rec))\n",
    "        field_blob: Dict[str, str] = {f: to_text(rec.get(f)) for f in available_fields}\n",
    "\n",
    "        for cls, per_field in AVOID_RULES.items():\n",
    "            # build the final field->keywords map (explicit + wildcard)\n",
    "            field_kws = materialize_field_keywords(per_field, available_fields)\n",
    "            if not field_kws:\n",
    "                continue\n",
    "\n",
    "            # test this job for this class\n",
    "            matched_fields: Dict[str, List[str]] = {}\n",
    "            for f, kws in field_kws.items():\n",
    "                blob = field_blob.get(f, \"\")\n",
    "                if not blob:\n",
    "                    continue\n",
    "                found = [kw for kw in kws if kw_boundary_search(blob, kw)]\n",
    "                if found:\n",
    "                    matched_fields[f] = sorted(set(found))\n",
    "\n",
    "            if matched_fields:\n",
    "                # we have at least one hit: add/update this class in output\n",
    "                bucket = hits_out.setdefault(cls, {\"job_ids\": [], \"matches\": {}})\n",
    "                if job_id not in bucket[\"job_ids\"]:\n",
    "                    bucket[\"job_ids\"].append(job_id)\n",
    "                    total_hits += 1\n",
    "                # store only non-empty matches for this job_id\n",
    "                bucket[\"matches\"][job_id] = matched_fields\n",
    "\n",
    "    # tidy/sort for stable diffs\n",
    "    for cls, bucket in list(hits_out.items()):\n",
    "        bucket[\"job_ids\"] = sorted(bucket[\"job_ids\"], key=lambda x: (len(str(x)), str(x)))\n",
    "        # if somehow a class ended up with no job_ids (shouldn't happen), drop it\n",
    "        if not bucket[\"job_ids\"]:\n",
    "            del hits_out[cls]\n",
    "\n",
    "    save_json(hits_out, OUTPUT_PATH)\n",
    "\n",
    "    print(f\"[OK] scanned {total} jobs from {DETAILS_PATH}\")\n",
    "    print(f\"[OK] wrote ONLY hits to {OUTPUT_PATH}\")\n",
    "    for cls in sorted(hits_out.keys()):\n",
    "        print(f\"  - {cls}: {len(hits_out[cls]['job_ids'])} job(s)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0486ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_details_db(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b43fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Python jobs: 169\n",
      "Total knowledge fullstack jobs: 25\n",
      "Total wanted Python jobs: 117\n",
      "- 1864545 | Senior Fabric IP Verification Engineer | ['Austin, Texas, United States + 4 more locations'] | 0-25 % | Sep 18, 2025 | https://jobs.careers.microsoft.com/global/en/job/1864545/\n",
      "- 1879713 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 23, 2025 | https://jobs.careers.microsoft.com/global/en/job/1879713/\n",
      "- 1857039 | Principal Software Engineer | ['Redmond, Washington, United States + 4 more locations'] | None | Sep 23, 2025 | https://jobs.careers.microsoft.com/global/en/job/1857039/\n",
      "- 1864645 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 23, 2025 | https://jobs.careers.microsoft.com/global/en/job/1864645/\n",
      "- 1828770 | Senior Applied Scientist-Word | ['Redmond, Washington, United States'] | 0-25 % | Sep 23, 2025 | https://jobs.careers.microsoft.com/global/en/job/1828770/\n",
      "- 1882171 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 23, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882171/\n",
      "- 1836776 | Software Engineer II - Virtualization and Kernel | ['Redmond, Washington, United States'] | None | Sep 23, 2025 | https://jobs.careers.microsoft.com/global/en/job/1836776/\n",
      "- 1881923 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881923/\n",
      "- 1861591 | Senior Software Engineer - Backend | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1861591/\n",
      "- 1882244 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882244/\n",
      "- 1882212 | Senior Software Engineer | ['Multiple Locations, United States'] | None | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882212/\n",
      "- 1878947 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1878947/\n",
      "- 1882147 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882147/\n",
      "- 1881919 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881919/\n",
      "- 1862283 | Lead AI Software Architect | ['Redmond, Washington, United States + 4 more locations'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1862283/\n",
      "- 1882132 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882132/\n",
      "- 1882817 | Software Engineering II | ['Redmond, Washington, United States'] | None | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882817/\n",
      "- 1881327 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881327/\n",
      "- 1882081 | Software Engineer II, Surface | ['Redmond, Washington, United States'] | None | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882081/\n",
      "- 1869160 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1869160/\n",
      "- 1881053 | Senior Software Engineer | ['Redmond, Washington, United States + 1 more location'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881053/\n",
      "- 1879796 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1879796/\n",
      "- 1857312 | Data Science: Internship Opportunities – Redmond | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1857312/\n",
      "- 1874255 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1874255/\n",
      "- 1881313 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881313/\n",
      "- 1879064 | Senior Verification Engineer | ['Raleigh, North Carolina, United States + 4 more locations'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1879064/\n",
      "- 1880869 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880869/\n",
      "- 1881111 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881111/\n",
      "- 1882156 | Senior Software Engineer | ['Multiple Locations, United States + 1 more location'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882156/\n",
      "- 1874705 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1874705/\n",
      "- 1854695 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1854695/\n",
      "- 1881950 | Principal Software Engineer | ['Redmond, Washington, United States'] | None | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881950/\n",
      "- 1881100 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881100/\n",
      "- 1879811 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1879811/\n",
      "- 1880670 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880670/\n",
      "- 1882154 | Principal Validation Manager | ['Mountain View, California, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882154/\n",
      "- 1881106 | PRINCIPAL SOFTWARE ENGINEER | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881106/\n",
      "- 1880104 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880104/\n",
      "- 1882267 | Software Engineer II (Networking & Telemetry Systems) | ['Mountain View, California, United States + 1 more location'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882267/\n",
      "- 1880968 | Software Engineer II - Full Stack | ['Redmond, Washington, United States + 1 more location'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880968/\n",
      "- 1881676 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881676/\n",
      "- 1880656 | Senior Software Engineer | ['Redmond, Washington, United States'] | None | Sep 24, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880656/\n",
      "- 1872506 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1872506/\n",
      "- 1852516 | Senior Software Engineer - Azure Networking | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1852516/\n",
      "- 1858569 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1858569/\n",
      "- 1854615 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1854615/\n",
      "- 1881837 | Senior Cloud Network Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881837/\n",
      "- 1881356 | Digital Cloud Solution Architecture | Azure | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881356/\n",
      "- 1880481 | Senior Software Engineer, Minecraft | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880481/\n",
      "- 1862877 | Senior Software Engineer | ['Redmond, Washington, United States'] | None | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1862877/\n",
      "- 1883009 | Applied Scientist II (CoreAI) | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1883009/\n",
      "- 1856943 | Software Engineer II | ['Redmond, Washington, United States'] | None | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1856943/\n",
      "- 1879479 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1879479/\n",
      "- 1841252 | Senior Software Engineer-Purview Information Protection | ['Atlanta, Georgia, United States'] | None | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1841252/\n",
      "- 1882182 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882182/\n",
      "- 1873390 | Senior Program Manager | ['Redmond, Washington, United States + 3 more locations'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1873390/\n",
      "- 1882314 | Senior Software Engineering Manager | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882314/\n",
      "- 1873243 | Principal Technical Program Manager | ['Redmond, Washington, United States + 3 more locations'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1873243/\n",
      "- 1873070 | Senior Design Verification Engineer | ['Mountain View, California, United States + 3 more locations'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1873070/\n",
      "- 1878935 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1878935/\n",
      "- 1883354 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1883354/\n",
      "- 1883330 | Software Engineer - Artificial Intelligence | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1883330/\n",
      "- 1849447 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1849447/\n",
      "- 1860402 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1860402/\n",
      "- 1872469 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1872469/\n",
      "- 1861814 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1861814/\n",
      "- 1882648 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 25, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882648/\n",
      "- 1830570 | Senior ML Research Engineer – LLM Quantization & Model Optimization | ['Mountain View, California, United States + 1 more location'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1830570/\n",
      "- 1881683 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881683/\n",
      "- 1863900 | Principal Software Engineer | ['Mountain View, California, United States + 1 more location'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1863900/\n",
      "- 1856745 | Software Engineer II | ['Redmond, Washington, United States'] | None | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1856745/\n",
      "- 1881575 | Senior Software Engineer | ['Redmond, Washington, United States'] | None | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881575/\n",
      "- 1880675 | Principal Applied AI Engineer | ['San Francisco, California, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880675/\n",
      "- 1861360 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1861360/\n",
      "- 1878955 | Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1878955/\n",
      "- 1878007 | Senior Compiler Engineer | ['Redmond, Washington, United States + 3 more locations'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1878007/\n",
      "- 1880525 | Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880525/\n",
      "- 1881032 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1881032/\n",
      "- 1852560 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1852560/\n",
      "- 1883845 | SENIOR SOFTWARE ENGINEER | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1883845/\n",
      "- 1859554 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1859554/\n",
      "- 1874727 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1874727/\n",
      "- 1874005 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1874005/\n",
      "- 1879217 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1879217/\n",
      "- 1883643 | Principal Design Engineer Manager - AI Network Silicon | ['Raleigh, North Carolina, United States + 3 more locations'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1883643/\n",
      "- 1882250 | Software Engineer II - Software Defined Networking | ['Multiple Locations, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882250/\n",
      "- 1851573 | Senior Software Engineer-Office for the Web | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1851573/\n",
      "- 1882231 | Software Engineer II | ['Redmond, Washington, United States'] | None | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882231/\n",
      "- 1849013 | Senior Software Engineer | ['Redmond, Washington, United States'] | None | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1849013/\n",
      "- 1875049 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1875049/\n",
      "- 1874914 | Software Engineer II- Microsoft Teams | ['Redmond, Washington, United States'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1874914/\n",
      "- 1880533 | Critical Environment Platform Engineer | ['Atlanta, Georgia, United States + 4 more locations'] | 0-25 % | Sep 26, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880533/\n",
      "- 1839980 | Software Engineer II - CTJ - Poly | ['Multiple Locations, United States + 4 more locations'] | 0-25 % | Sep 27, 2025 | https://jobs.careers.microsoft.com/global/en/job/1839980/\n",
      "- 1882860 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 27, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882860/\n",
      "- 1875607 | Senior Software Engineer | ['Mountain View, California, United States + 1 more location'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1875607/\n",
      "- 1883645 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1883645/\n",
      "- 1852186 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1852186/\n",
      "- 1842932 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1842932/\n",
      "- 1864253 | Senior Software Engineer | ['Multiple Locations, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1864253/\n",
      "- 1884153 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1884153/\n",
      "- 1882646 | Senior Data Scientist -Media Optimization | ['Redmond, Washington, United States + 3 more locations'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1882646/\n",
      "- 1884156 | Senior Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1884156/\n",
      "- 1884120 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1884120/\n",
      "- 1884141 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1884141/\n",
      "- 1861505 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1861505/\n",
      "- 1842582 | Software Engineer II - Infrastructure | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1842582/\n",
      "- 1819624 | Principal Software Engineer | ['Redmond, Washington, United States'] | None | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1819624/\n",
      "- 1833803 | Software Engineer II | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1833803/\n",
      "- 1876065 | Principal Software Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1876065/\n",
      "- 1858310 | Software Engineer | ['Atlanta, Georgia, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1858310/\n",
      "- 1885079 | Software Engineer | ['Redmond, Washington, United States'] | None | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1885079/\n",
      "- 1848031 | Principal Design Engineer | ['Raleigh, North Carolina, United States + 1 more location'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1848031/\n",
      "- 1880529 | Critical Infrastructure Network Engineer | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1880529/\n",
      "- 1877977 | Senior Software Engineer | ['Redmond, Washington, United States'] | None | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1877977/\n",
      "- 1864732 | Principal Engineering Manager | ['Multiple Locations, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1864732/\n",
      "- 1870870 | Senior Applied Scientist | ['Redmond, Washington, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1870870/\n",
      "- 1861552 | Software Engineer II | ['Multiple Locations, United States'] | 0-25 % | Sep 29, 2025 | https://jobs.careers.microsoft.com/global/en/job/1861552/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# show knowledge python hits\n",
    "python_job = set(df['knowledge_python']['job_ids'])\n",
    "knowledge_fullstack = set(df['knowledge_fullstack']['job_ids'])\n",
    "clearance_required = set(df['clearance_required']['job_ids'])   \n",
    "visa_sponsorship_block = set(df['visa_sponsorship_block']['job_ids'])\n",
    "unwanted_positions = set(df['unwanted_positions']['job_ids'])\n",
    "senior_only = set(df['senior_only']['job_ids'])\n",
    "wanted_python_jobs = python_job - knowledge_fullstack - clearance_required - visa_sponsorship_block - unwanted_positions - senior_only\n",
    "print(f\"Total Python jobs: {len(python_job)}\")\n",
    "print(f\"Total knowledge fullstack jobs: {len(knowledge_fullstack)}\")\n",
    "print(f\"Total wanted Python jobs: {len(wanted_python_jobs)}\")\n",
    "full_list = load_details_db(DB_PATH_OUT)\n",
    "# Sort wanted_python_jobs by date_posted (oldest first)\n",
    "\n",
    "def parse_date(date_str):\n",
    "    # Try to parse various date formats, fallback to a large date if missing\n",
    "    if not date_str:\n",
    "        return datetime.max\n",
    "    for fmt in (\"%b %d, %Y\", \"%Y-%m-%d\", \"%b %d, %Y.\"):\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), fmt)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return datetime.max\n",
    "\n",
    "sorted_jobs = sorted(\n",
    "    wanted_python_jobs,\n",
    "    key=lambda job_id: parse_date(full_list.get(job_id, {}).get(\"date_posted\"))\n",
    ")\n",
    "\n",
    "full_detailed_list = load_details_db(DETAILS_PATH)\n",
    "\n",
    "sorted_jobs_detailed = sorted(\n",
    "    wanted_python_jobs,\n",
    "    key=lambda job_id: parse_date(full_detailed_list.get(job_id, {}).get(\"date_posted\"))\n",
    ")\n",
    "\n",
    "for job_id in sorted_jobs:\n",
    "    job = full_list.get(job_id, {})\n",
    "    print(f\"- {job_id} | {job.get('title')} | {job.get('locations')} | {job.get('travel')} | {job.get('date_posted')} | {job.get('url')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98119ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1 jobs to jobs_by_date\\jobs_18_september_2025.json\n",
      "Saved 6 jobs to jobs_by_date\\jobs_23_september_2025.json\n",
      "Saved 35 jobs to jobs_by_date\\jobs_24_september_2025.json\n",
      "Saved 25 jobs to jobs_by_date\\jobs_25_september_2025.json\n",
      "Saved 25 jobs to jobs_by_date\\jobs_26_september_2025.json\n",
      "Saved 2 jobs to jobs_by_date\\jobs_27_september_2025.json\n",
      "Saved 23 jobs to jobs_by_date\\jobs_29_september_2025.json\n",
      "\n",
      "Total files created: 7\n",
      "Total jobs saved: 117\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Group jobs by date_posted\n",
    "jobs_by_date = defaultdict(list)\n",
    "\n",
    "for job_id in sorted_jobs_detailed:\n",
    "    job = full_detailed_list.get(job_id, {})\n",
    "    date_posted = job.get('date_posted', 'unknown')\n",
    "    \n",
    "    # Create a clean filename from date\n",
    "    if date_posted and date_posted != 'unknown':\n",
    "        try:\n",
    "            # Parse the date and format it for filename\n",
    "            parsed_date = parse_date(date_posted)\n",
    "            if parsed_date != datetime.max:\n",
    "                # Format as \"18_september_2025\"\n",
    "                filename_date = parsed_date.strftime(\"%d_%B_%Y\").lower()\n",
    "            else:\n",
    "                filename_date = date_posted.replace(\"-\", \"_\").replace(\" \", \"_\").replace(\",\", \"\")\n",
    "        except:\n",
    "            filename_date = date_posted.replace(\"-\", \"_\").replace(\" \", \"_\").replace(\",\", \"\")\n",
    "    else:\n",
    "        filename_date = \"unknown_date\"\n",
    "    \n",
    "    jobs_by_date[filename_date].append({\n",
    "        \"job_id\": job_id,\n",
    "        \"title\": job.get('title'),\n",
    "        \"locations\": job.get('locations'),\n",
    "        \"travel\": job.get('travel'),\n",
    "        \"date_posted\": date_posted,\n",
    "        \"url\": job.get('url'),\n",
    "        \"required_qualifications_text\": job.get('required_qualifications_text'),\n",
    "        \"preferred_qualifications_text\": job.get('preferred_qualifications_text'),\n",
    "        \"other_requirements_text\": job.get('other_requirements_text'),\n",
    "        \"pay_ranges\": job.get('pay_ranges')\n",
    "    })\n",
    "\n",
    "# Save each date group to a separate file\n",
    "output_dir = \"jobs_by_date\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for date_str, jobs_list in jobs_by_date.items():\n",
    "    filename = f\"jobs_{date_str}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_list, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(jobs_list)} jobs to {filepath}\")\n",
    "\n",
    "print(f\"\\nTotal files created: {len(jobs_by_date)}\")\n",
    "print(f\"Total jobs saved: {sum(len(jobs) for jobs in jobs_by_date.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d505d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-scrapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
